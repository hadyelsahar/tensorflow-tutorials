{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Char RNN using Tensorflow\n",
    "\n",
    "This is a sample code from stanford [tensorflow course](http://web.stanford.edu/class/cs20si/syllabus.html) about building a character RNN for Arxiv abstract generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## imports \n",
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation\n",
    "## Static preparing of datasets \n",
    "\n",
    "- no tensorflow involved and no need to run sessions for this part\n",
    "- reading arxiv abstracts line by line from a dataset and return sequence of vocabulary ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# character vocabulary\n",
    "vocab = (\" $%'()+,-./0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZ\\\\^_abcdefghijklmnopqrstuvwxyz{|}\")\n",
    "\n",
    "# Length of each sequence\n",
    "SEQ_LENGTH = 50 \n",
    "BATCH_SIZE = 64\n",
    "VOCAB_SIZE = len(vocab)\n",
    "\n",
    "def vocab_encode(text, vocab, oov=0):\n",
    "    return [vocab.index(x) + 1 if x in vocab else oov for x in text]\n",
    "\n",
    "def vocab_decode(array, vocab, oov=\"_oov_\"):\n",
    "    return ''.join([vocab[x - 1] if x > 0 else oov for x in array])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_data(filename, vocab=vocab, window=SEQ_LENGTH, overlap=SEQ_LENGTH/2):\n",
    "    \"\"\"\n",
    "    read abstract file line by line and chunk each line to some chunks\n",
    "    pad with zeros if the chunk is less than the window\n",
    "    \"\"\"\n",
    "    for text in open(filename):\n",
    "        text = vocab_encode(text, vocab)\n",
    "        for start in range(0, len(text) - window, int(overlap)):\n",
    "            chunk = text[start: start + window]\n",
    "            chunk += [0] * (window - len(chunk))\n",
    "            yield chunk\n",
    "            \n",
    "\n",
    "def read_batch(stream, batch_size=BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    read a stream of chunks from read_data and make them in batches to feed an RNN\n",
    "    \"\"\"\n",
    "    batch = [] \n",
    "    \n",
    "    for i in stream:\n",
    "        batch.append(i)\n",
    "        if len(batch) == batch_size:\n",
    "            yield batch \n",
    "            batch = [] \n",
    "            \n",
    "    yield batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF dataset preparation \n",
    "\n",
    "- dataset preparation in tensorflow session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_onehot(seq):\n",
    "    \"\"\"\n",
    "    seq: input sequence of size BATCH_SIZE X SEQ_LENGTH    \n",
    "    \"\"\"\n",
    "    \n",
    "    # convert sequence into 1 hot representations of size BATCH_SIZE X SEQ_LENGTH X VOCAB_SIZE\n",
    "    seq = tf.one_hot(seq, VOCAB_SIZE)\n",
    "    \n",
    "    # length tensor will be of size  BATCH_SIZE x 1 containing true lengths of each item in the batch at runtime\n",
    "    lengths = tf.reduce_sum(tf.reduce_max(tf.sign(seq),2), 1)\n",
    "    \n",
    "    return seq, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Models\n",
    "\n",
    "## Char RNN, loss function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def create_rnn(seq, length, state_sizes, num_layer = 3):    \n",
    "    \"\"\"\n",
    "     :param seq: batch of sequences of 1 hot representations of chars each sequence is of size \n",
    "                 BATCH_SIZE X SEQ_LENGTH X VOCAB_SIZE\n",
    "     :param length: true lengths of sequences in each batch size :  BATCH_SIZE X 1 \n",
    "     :param state_size:  size of the hidden state of the RNN\n",
    "    \"\"\"\n",
    "    \n",
    "    cells = [tf.nn.rnn_cell.GRUCell(state_size) for state_size in state_sizes]\n",
    "    \n",
    "    initial_state_placeholder = tuple([\n",
    "        tf.placeholder_with_default(input = cell.zero_state(tf.shape(seq)[0], tf.float32), shape = [None, state_sizes[i]])\n",
    "        for i, cell in enumerate(cells)\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell(cells, state_is_tuple=True)\n",
    "    \n",
    "    rnn_outputs, output_state = tf.nn.dynamic_rnn(cell, seq, length, initial_state_placeholder)\n",
    "    \n",
    "    return rnn_outputs, initial_state_placeholder, output_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_model(seq_placeholder, global_step, vocab_size, state_sizes, learning_rate):\n",
    "    \n",
    "    # converting seq to one hot vectors of size BATCH_SIZE X SEQ_LENGTH X VOCAB_SIZE\n",
    "    seq, lengths = create_onehot(seq_placeholder)\n",
    "    \n",
    "    rnn_outputs, initial_state_placeholder, out_state = create_rnn(seq, lengths,state_sizes=state_sizes)\n",
    "    \n",
    "    # logits of size BATCH_SIZE X SEQ_LENGTH X VOCAB_SIZE\n",
    "    logits = tf.contrib.layers.fully_connected(rnn_outputs, vocab_size, activation_fn=None)\n",
    "    \n",
    "    # the RNN is supposed to predict the next character \n",
    "    # i.e. predict the next 1 hot vector over the vocabulary\n",
    "    # each predicted label should match the next word in the sequence\n",
    "    predicted = logits[:,:-1]\n",
    "    labels = seq[:,1:]\n",
    "    \n",
    "    \"\"\"\n",
    "    `softmax_cross_entropy_with_logits`\n",
    "    This op expects unscaled logits, since it performs a `softmax` on `logits` internally for efficiency.  \n",
    "    Do not call this op with theoutput of `softmax`, as it will produce incorrect results.\n",
    "    \"\"\"\n",
    "    # the loss is the sum of the cross entropy across the batch \n",
    "    loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits=predicted, labels=labels))\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    \n",
    "    return loss, optimizer, initial_state_placeholder, out_state, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Char Generator \n",
    "generate the next character given an input character "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_generator(logits):\n",
    "    \"\"\"\n",
    "    logits: are the logits of the output of the RNN model\n",
    "    \"\"\"\n",
    "    \n",
    "    temp_placeholder = tf.placeholder(tf.float32, name=\"temperature\")    \n",
    "    sample = tf.multinomial(tf.exp(logits[:, -1] / temp_placeholder), 1)[:, 0] \n",
    "    \n",
    "    return sample, temp_placeholder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Generation functions \n",
    "### Create Training functions for the RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LOG_EVERY = 40\n",
    "\n",
    "def train_model(datafeeder, seq_placeholder, init_state_placeholder, out_state, loss, optimizer, sample, temp_placeholder):\n",
    "    \n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        iteration = 0 \n",
    "                \n",
    "        for batch in datafeeder:\n",
    "            \n",
    "            _, batch_loss =sess.run([optimizer, loss], feed_dict={seq_placeholder:batch})\n",
    "            \n",
    "                        \n",
    "            if (iteration + 1) % LOG_EVERY == 0:\n",
    "                print('iter{} : loss{}'.format(iteration, batch_loss))\n",
    "                print(generate_sentence(sess, seq_placeholder, init_state_placeholder, sample, out_state, temp_placeholder))\n",
    "                            \n",
    "            iteration +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Generation function of the RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TEMPRATURE = 0.7\n",
    "LEN_GENERATED = 300\n",
    "\n",
    "def generate_sentence(sess, seq_placeholder, init_state_placeholder, sample, out_state, temp_placeholder, len_generated=LEN_GENERATED, vocab=vocab):\n",
    "    \"\"\"\n",
    "    Generate sentence based on the previous character\n",
    "    \n",
    "    :param sess: session \n",
    "    :param vocab: vocabulary to generate from \n",
    "    :param char: previous character in the sequence  e.g. \"T\"\n",
    "    :param sample: sample from the distribution of the output\n",
    "    :param in_state: input state to the rnn \n",
    "    :param out_state: output state of the rnn \n",
    "    :param temp: temperature of the sampling    \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    \n",
    "    state = None \n",
    "    \n",
    "    # select a random seed from the vocabulary\n",
    "    # todo : fix\n",
    "    seed = \"T\"\n",
    "    sentence = \"\"\n",
    "    \n",
    "    for i in range(len_generated):\n",
    "        \n",
    "        batch = [vocab_encode(seed, vocab)]\n",
    "        \n",
    "        if state is not None:\n",
    "            feed = {\n",
    "                init_state_placeholder: state,\n",
    "                seq_placeholder:batch,\n",
    "                temp_placeholder: TEMPRATURE\n",
    "            }\n",
    "        else:\n",
    "            feed = {\n",
    "                seq_placeholder:batch,\n",
    "                temp_placeholder: TEMPRATURE                \n",
    "            }\n",
    "    \n",
    "        \n",
    "        index, state = sess.run([sample, out_state], feed_dict = feed)\n",
    "        sentence += vocab_decode(index, vocab)\n",
    "        seed = sentence[-1]\n",
    "        \n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN the RNN MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Variables\n",
    "STATE_SIZES = [200]\n",
    "LR = 0.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter39 : loss9567.9296875\n",
      "ITt                                                              e     e   e   e  e  e  e  e  e  e  e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e \n",
      "iter79 : loss8543.015625\n",
      "$je the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the \n",
      "iter119 : loss7483.62744141\n",
      "he the s an the the the the the s an the the the the s an the the s an the the the the the the s an the the the the the the the s an the the s an the the the s an the the the the the the the the the s an the the the s an the the s an the s an the the the s an the the the the the the the the s an the\n",
      "iter159 : loss6994.41748047\n",
      "he the for arale the for arale the for arale the for arale the for arale the for arale the for arale the for arale the for arale the for arale the for arale the for arale the for arale the for arale the for arale the for arale the for arale the for arale the for arale the for arale the for arale the\n",
      "iter199 : loss6551.06738281\n",
      "he the senter the senter the senter and ation the senter and ation the senter and ation the senter and ation the senter and ation the senter and ation the senter and ation the senter and ation the senter and ation the senter and ation the senter and ation the senter and ation the senter and ation th\n",
      "iter239 : loss6108.64648438\n",
      "he the semple and and a deep network and a deep network and a deep network and a deep network and a deep network and a deep network and a deep network and a deep network and a deep network and a deep network and a deep network and a deep network and a deep network and a deep network and a deep netwo\n",
      "iter279 : loss6039.51269531\n",
      "he propesting the propesting the propesting the propesting the propesting the propesting the propesting the propesting the propesting the propesting the propesting the propesting the propesting the propesting the propesting the propesting the propesting the propesting the propesting the propesting t\n",
      "iter319 : loss6014.00390625\n",
      "he and the eralization and the eralization and the eralization and the eralization and the eralization and the eralization and the eralization and the eralization and the eralization and the eralization and the eralization and the eralization and the eralization and the eralization and the eralizati\n",
      "iter359 : loss5412.03027344\n",
      "he architect on the sempers and a propose semplome the sempers of the semper that the sempers of the semper that the sempers of the semper that the sempers of the semper that the sempers and the sempers of the semper that the sempers of the semper that the sempers and the sempers and the sempers and\n",
      "iter399 : loss5431.76318359\n",
      "he and explore the araly in out proples and and explore the araly in out proples and and explore the araly in out proples and and explore the araly in out proples and and explore the araly in out proples and and explore the araly in out proples and and explore the araly in out proples and and explor\n",
      "iter439 : loss4661.15917969\n",
      "he for the and the problem in the distriction of the and the problem in the distriction of the and the problem in the distriction of the problem in the distriction of the and the problem in the distriction of the and the problem in the distriction of the problem in the distriction of the and the pro\n",
      "iter479 : loss4581.34375\n",
      "he a simel a deep neural networks (DNN) and a sumples a proposed a significant a sures for a propase a proposed a proposed a simel a deep neural networks (DNN) and a sumples a proposed a proposed a significant a simple neural networks (DNN) and a sumples a proposed a significant a simple neural netw\n",
      "iter519 : loss4489.75976562\n",
      "he convex the networks that the convex the networks that the convex the networks that the convex the networks that the convex the networks that the convex the networks that the convex the networks that the convex the networks that the convex the networks that the convex the networks that the convex \n",
      "iter559 : loss4727.99951172\n",
      "he and the sumpless of the and the sumpless of the and the sumpless of the and the sumpless of the and the sumpless of the and the sumpless of the and the sumpless of the and the sumpless of the and the sumpless of the and the sumpless of the and the sumpless of the and the sumpless of the and the s\n",
      "iter599 : loss4238.02539062\n",
      "he for the the network are and the network are and the network are and the network are and the network are and the network are and the network are and the network are and the network are and the network are and the network are and the network are and the network are and the network are and the netwo\n",
      "iter639 : loss4277.66113281\n",
      "he a simple and stochastic gradient descent and stochastic gradient descent and stochastic gradient descent and stochastic gradient descent and stochastic gradient descent and stochastic gradient descent and stochastic gradient descent and stochastic gradient descent and stochastic gradient descent \n",
      "iter679 : loss3827.58398438\n",
      "he and the speech recognition to train a new and state-of-the-art deep learning and the speech recognition to train a new and state-of-the-art deep learning and the speech recognition to train a new and state-of-the-art experiments and the speech recognition to train a new and state-of-the-art exper\n",
      "iter719 : loss3712.0559082\n",
      "he architecture of the architecture of the architecture of the architecture of the architecture of the architecture of the architecture of the architecture of the architecture of the architecture of the architecture of the architecture of the architecture of the architecture of the architecture of t\n",
      "iter759 : loss4186.93066406\n",
      "he and the problem in the problem in the problem in the problem in the problem in the problem in the problem in the problem in the problem in the problem in the problem in the problem in the problem in the problem in the problem in the problem in the problem in the problem in the problem in the prob\n",
      "iter799 : loss3773.13085938\n",
      "he architecture of the search for and computations with the existing deep learning requires deep learning recent results on the computations we propose a new architecture of the search for and computations with the existing deep learning requires deep learning recent results on the computations we p\n",
      "iter839 : loss3768.22607422\n",
      "he convergence transformations and the network proposed method for deep neural network architectures structure of the network proposed method for deep neural network architectures structure of the network proposed method for deep neural network architectures structure of the network proposed method \n",
      "iter879 : loss3597.48291016\n",
      "he and the problem in the desirible to the context of the deep reneralized by the different distribution in the context of the deep reneralized by the different distribution in the context of the deep reneralized by the different on the context of the deep reneralized by the different on the context\n",
      "iter919 : loss3541.58300781\n",
      "he activation from the such as a set of the such as a set of the such as a set of the such as a set of the such as a set of the such as a set of the such as a set of the such as a set of the such as a set of the such as a set of the such as a set of the such as a set of the such as a set of the such\n",
      "iter959 : loss3574.67211914\n",
      "he architecture of the network architecture of the network architecture of the network architecture of the network architecture of the network architecture of the network architecture of the network architecture of the network architecture of the network architecture of the network architecture of t\n",
      "iter999 : loss3770.32617188\n",
      "he accuracy of the speech recognition to the existing settorn structure deep networks (DNN) to the of the action architecture of the action architecture of the action architecture of the action architecture of the action architecture of the action architecture of the action architecture of the actio\n",
      "iter1039 : loss3530.56835938\n",
      "he approach is to implemented by the network in a simple machine layer of the network in a simple machine layer of the network in a simple machine layer of the network in a simple machine layer of the network in a simple machine layer of the network in a simple machine layer of the network in a simp\n",
      "iter1079 : loss3454.23632812\n",
      "hese search for analysis and then experiments learning to be adversaries for the long structure convergence rates of a deep learning (ase to the different of the larger signals and the desirible to simple formal convergence rates of a deep learning (ase to the different of the larger signals and the\n",
      "iter1119 : loss3366.51855469\n",
      "he activation of the activation of the activation of the activation of the activation of the activation of the activation of the activation of the activation of the activation of the activation of the activation of the activation of the activation of the activation of the activation of the activatio\n",
      "iter1159 : loss3435.99462891\n",
      "he convolutional neural networks are and optimization processing and successful networks and a the recugrent neural networks and representation layers and show that the network depth and sumparally some in order to a concodduned by a fully show that the network depth and sumparally some in order to \n",
      "iter1199 : loss3118.53076172\n",
      "he accuracy of the input and of the training deep neural networks (DNN) have achieve the the to a simple and of the training deep neural networks (DNN) have achieve the the to a simple and of the training deep neural networks (DNN) have achieve the the to a simple and of the training deep neural net\n",
      "iter1239 : loss3388.91113281\n",
      "he architectures for gradient descent gradient desting the complexity indutations of the architectures for gradient descent gradient desting the complexity indutations of the architectures for gradient descent gradient desting the complexity indutations of the architectures for gradient descent grad\n",
      "iter1279 : loss3033.33789062\n",
      "he architecture convolutional neural networks (DNNs) are sumporded to a deep neural networks to simple and computation systems for the network is the context of deep neural networks to simple and computation systems for the network is the context of deep neural networks to simple and computation sys\n",
      "iter1319 : loss3214.98950195\n",
      "he approaches are in the prediction and interest in a single models of the input scale-grations of computational and existing methods for each using an an of employs for example, by the existing methods for each using speedup of and maximally successful between layers that are groduced and computer \n",
      "iter1359 : loss3052.03369141\n",
      "he network is of the success of the success of the success of the success of the success of the success of the success of the success of the success of the success of the success of the success of the success of the success of the success of the success of the success of the success of the success o\n",
      "iter1399 : loss3034.20947266\n",
      "he and of the network is a method of the network is a method of the network is a method of the network is a method of the network is a method of the network is a method of the network is a method of the network is a method of the network is a method of the network is a method of the network is a met\n",
      "iter1439 : loss3163.40161133\n",
      "he accurace for the training data using algorithms to apply dected by the activation from the activation from the activation from the activation from the activation from the activation from the activation from the activation from the activation from the activation from the activation from the activa\n",
      "iter1479 : loss3215.46044922\n",
      "he architecture that the network architectures and show that the network architectures and show that the network architectures and show that the network architectures and show that the network architectures and show that the network architectures and show that the network architectures and show that\n",
      "iter1519 : loss3275.35913086\n",
      "he discriminate better speedup us the discriminate between the size of the size of the size of the size of the size of the size of the size of the size of the size of the size of the size of the size of the size of the size of the size of the size of the size of the size of the size of the size of t\n",
      "iter1559 : loss3111.73120117\n",
      "he computational and transformation of the approach is computationally effectiveless of simple model as whelated to train a simple problem is work functions in the complexity interpretations of the success using deep neural network (RNNs) as the representation and the proposed $L_p$ unit in speech r\n",
      "iter1599 : loss3210.12817383\n",
      "he convolutional layer-wise approximation processing a convex layer of indep neural networks to model by the and of the network in computational staptional neural network constructed in the convolutional layer-wise pre-training and the network in each layer-order representation that a deep neural ne\n",
      "iter1639 : loss2763.91967773\n",
      "he achieve an approach for distribution is the anding on the training data from the pre-training the achieve an approach for distribution for deep networks in a deep networks in a deep networks in a deep networks in a deep networks in a deep networks in a deep networks in a deep networks in a deep n\n",
      "iter1679 : loss2717.11083984\n",
      "he networks and representations for deep neural networks. We explore they are approximated in the recognizenge in a simple connectioning stacking and stability proposed methods for depensentations of the system is an employs for the system is an embedding the effectiveness of our proposed by the exi\n",
      "iter1719 : loss2962.64453125\n",
      "he conventional constrained to stare of the network is a single layers of a deep neural networks (DNNs) are available local conventional architecture can be a simple layers of a deep neural networks (DNNs) are available local conventional architecture can be a simple layers of a deep neural networks\n",
      "iter1759 : loss2927.65820312\n",
      "he accurace tasks of an existing set of existing methods to an experiments led to the computation on small DNN. This datacan structures such as the approach of learning and the approach of learning algorithm for the input samples: in a different computationally intensive to activel as some of the pa\n",
      "iter1799 : loss2784.859375\n",
      "he network architecture of a parameter search for expernsentions are general particular, to the success of deep neural networks that are aments a neural networks that can be used to the such learning parameters and the success of deep neural networks that are aments a neural networks that can be use\n",
      "iter1839 : loss2891.58496094\n",
      "he accuracy as standard deep learning rate while invalis and the are and of the input sample from the pooling operation and the input sample from the posterior results on a novel deep learning algorithms for training of neural network in a different distribution is ever composent of the input sample\n",
      "iter1879 : loss2435.05957031\n",
      "he approach to train a gradient descent tasks to a set of experiments encoder and the success of the system is a simple convergence rate contralt distribution to train a small experiments success of examples by the deep networks (DNNs). The space of the system is a simple convergence rate contralt d\n",
      "iter1919 : loss2554.02441406\n",
      "he contributed with the network architectures based on and in computation in the cont our model as the network architectures been selicience of the network increases the architecture of the network architectures based on and in computation in the cont our model as the network architectures been seli\n",
      "iter1959 : loss3100.64282227\n",
      "he analysis of the learning machines structures and the problem data from ears lor-tore the training convex ERMs in the from the problem data from ears lor-tore the training convex ERMs in the from the problem data from ears lor-tore the training convex ERMs in the training complexity of the trainin\n",
      "iter1999 : loss2707.17333984\n",
      "he network is a simple complex on object representations for deepents sencesons redections of the realize content of a concept of results on a stacked RNNs hape bencemer bounds the architecture encoder based on a parallel capputations of the recently proposed deep neural networks (DNN) have been sho\n",
      "iter2039 : loss2764.37866211\n",
      "he convolutional neural networks (DNNs) as a set of deep neural networks (DNNs) as a set of deep neural networks (DNNs) as a set of deep neural networks (DNNs) as a set of deep neural networks (DNNs) as a set of deep neural networks (DNNs) as a set of deep neural networks (DNNs) as a set of deep neu\n",
      "iter2079 : loss2662.27270508\n",
      "he accuracy for speech recogniters enticative as partidue to sempetitive with conventional and the proposed GF-RNN and introduce a novel convergence preserving in the deep learning algorithm is to decods on the best of the benefit the context of deep networks can a contex using the recurrent neural \n",
      "iter2119 : loss2329.52392578\n",
      "he network architectures by a functions that the embedding that recognition in the sources for training deep neural networks (DNNs) as the effectiveness of a method is embeds the state-of-the-art performance of a method of expensive to the embedding that recognition in the sources for training deep \n",
      "iter2159 : loss2685.85546875\n",
      "he convergence tach can be used to find distille network structure of the input samples by a function of the network is a recornition classification machines) convergence than SparkNet of the network parameters and converges from pre-training of the network parameters and convergence than SparkNet. \n",
      "iter2199 : loss2714.87988281\n",
      "he consider the system compared to state-of-the-art deep networks that can be used to train the context of the recurrent layers convergence and stales moder for gradients show that the proposed GF-RNN wn these learned computation and the proposed method models have been challenging results on MNIST \n",
      "iter2239 : loss2618.70654297\n",
      "he control of a set of precision local learning rate and a particularly results on a simple computation in training a deep neural networks (DNNs) as particular to the success to deep neural network (DNN) models (in). We proposed deep neural networks (DNNs) as standard $\\lang a theoretical analyzing \n",
      "iter2279 : loss2758.36035156\n",
      "he accurace frem that our approach is signification performance on sove out an experiments show that our approach is straints in the deep learning algorithms for sequence and the desired theoretically and they are an approximate stares in and in a given our methods of computation beoteposed in the p\n",
      "iter2319 : loss2459.37109375\n",
      "he experimentally desirable training deep networks that be matrix allow design dropout high-likerameness of a method for sparse pathway and the effectiveness of a method for the higher order representation that the network architecture of the succession layers, and the order this work with maximum l\n",
      "iter2359 : loss2748.18774414\n",
      "he convolutional networks (DNN) have been the individua nondence activation functions that is not feedgot learned with the state-of-the-art deep learning algorithms have been speedup of the network can be mulling the reasons for the input faster that a deep neural networks are able to better results\n",
      "iter2399 : loss2392.87329102\n",
      "he decoder and increases the model for the local desting to a deep networks and show that the full into the successful as precesss of the layer generatorsist to ted or impotting the context of deep networks in a single layer is and stand techniques and for distribution from the successful to teclure\n",
      "iter2439 : loss2572.13500977\n",
      "he framework allows the space of learning releting and standard a theoretical analyzing speech recognition systems with maximum likel-ordinting recently interpartatals for tasts that the of the art on some successful by the sependion for the such as objectives that are a the set it is not used in sp\n",
      "iter2479 : loss2479.68774414\n",
      "he deep networks can be achieved the arting the network depth such as the deep learning architecture of the network depth and subsors the distribution in which the distributed to be reduce speedup due to the distributed to be reduce speedup due to the distributed to be reduce speedup due to the dist\n",
      "iter2519 : loss2377.10009766\n",
      "he decompose the constraints are under non-negative and may bo large than 8% relative development on the RNN (SBM) as the deviles dropout have been the desired the easier to decoder that introduce a small model consiste-criability. We explore thon or a state of the art on the network depth and stand\n",
      "iter2559 : loss2449.98754883\n",
      "he network for their success in the state-of-the-art performance on a parametrization of the approachis work, we introduce a simple configuration of the targets are such as providing a new prucestion to the training of the training and and of the training and and of the training and and of the train\n",
      "iter2599 : loss2483.6628418\n",
      "he accuracy computational matrix factorization and introduce a new approach to trained on inelicel with an under to improvement and invalient on a novel frameworks learning matriles performance on the network into an inference are interaction with the ary finat the successis points of large scales d\n",
      "iter2639 : loss2494.91479492\n",
      "he network computes a well represent the recognition with a simple preserving in the context of a large DNNs works learning rate and achieves the amount of complex task, which we explore the model to addrapt on a new approach compared to a deep neural network acoustic models using simple from standa\n",
      "iter2679 : loss2521.20556641\n",
      "he network with a convengence scale and compound a parameter variants. The generated by the approach is common learning the compositional networks (DNN) to predict the network with and probabilistic data problems data. They learn to perform a convex lates. We ackies a 3.47 WER on the architecture th\n",
      "iter2719 : loss2563.15771484\n",
      "he demonstrate the successive stopal demands, structure, deep neural networks (DNN) to access of the learning procedure. We construct of learning and standing matrix machine learning tasks that is an a better performance in a simple different distribution in a simple different distribution in a simp\n",
      "iter2759 : loss2506.94750977\n",
      "he network computer with challenging the prediction and computationally effective methods of state-of-the-art performance on a speech recognition systems with minimal orbits. Intuitively, these features are in the proposed $L_p$ units on a speech recognition systems with minimal orbits. Intuitively,\n",
      "iter2799 : loss2664.01708984\n",
      "he deep RNNs are requiring speech recognition techniques provides a setting, our algorithms to hier-400-network for the input showing to device result information succ standard possind of the network depth such ancoppross oven the convolutional networks and have a deep convolutional layer-wise pre-t\n",
      "iter2839 : loss2258.24414062\n",
      "he different convolutional networks (DBNs) recoorsity compare or better prediction can be achieve the system divermence and maximal astimizis) and the desired the network depth and show that build non-convex loss function to the only a large using s gradient classified the effect of stacked RNN mode\n",
      "iter2879 : loss2221.52587891\n",
      "he network connection with the system to provide a backpropagation or results of regularization (e.g. These we obtervicill by a frameage machine learning to computation of and state-of-the-art results on a neural networks (DNNs) as can networks (RNNs) are additive accies aundems a novel form the suc\n",
      "iter2919 : loss2528.90771484\n",
      "he conventional neural networks in the conventional neural networks in the conventional neural networks in the conventional neural networks in the conventional neural networks in the conventional neural networks in the conventional neural networks in the conventional neural networks in the conventio\n",
      "iter2959 : loss2313.19189453\n",
      "he classification alled to train and compared to the succession layers of dropout, we are under neural networks (DNN) have demonstrated to train mechanisming deep networks (DNN) have dropout exact unitworks, which comprise the deep layers to the outperform DNN cluster's can despite the components al\n",
      "iter2999 : loss2314.68115234\n",
      "he network increases the about the network architecture of the network architectures by high-likely a predictive method to predict that the network architecture that allo lateer neural networks (RNNs) have benefict to respect to the network architecture that all structure, computation in neural netw\n",
      "iter3039 : loss2415.05493164\n",
      "he deep learning architecture that is important parallel simples for the linear and stochastic gradient descent is a relattond of deep neural networks (DNN) to statistrcation of the network is an improvements in the linear and deep networks of learning tasks and successful the learning parallelisg. \n",
      "iter3079 : loss2076.23876953\n",
      "he network with that the network is of the input of the art deep networks (DNN) to the serves design incruding recently, shandard RNN dropsucting the corrent a novel standations to further controls for the input of the training deep neural networks that can be empirical ypeeis the network is a simpl\n",
      "iter3119 : loss2191.92529297\n",
      "he approach further improvements in we explore the pooling recognition in the training a general complexity and show that the network are likite. The path an optimimated by the and then emportic tisk we show that these speech recognition with a simple contribute sheech and the network depth for conv\n",
      "iter3159 : loss2452.73754883\n",
      "he different the convergence rates of some to analyze compare ditterat experiments showe standard RNNs (CD) hidden layer is trained on MBN acoustic model structure that allows in a standard Rprop's of the experiments ledet to the context of the input shape. Reserved interesting in a stacked RNN) arc\n",
      "iter3199 : loss2263.57568359\n",
      "he network with model parameters and computationally effertions for implementations of parameters to respect a novel formalize of recently, the architecture of the seaves per performance in order to perform a creition of recently, making the training segsentation of a deep neural networks (DNNs) as \n",
      "iter3239 : loss2390.68847656\n",
      "he demonstrate the parallelized to be methods  this popen, we propose a new stochastic model of stochastic modeling of the main results on the parallel convolutional neural networks are computational complexity of deep neural networks are conventional neural networks are computational complexity of \n",
      "iter3279 : loss2240.84130859\n",
      "he classification allow in learning problems divections and can be shown that in in taken from approaches are all a single layer to form that our algorithm which SVRG takes as a simple preserving data to some to an of the network withon layers to dropding a set of spachionally different architecture\n",
      "iter3319 : loss1902.83740234\n",
      "he network is a transformation, and show that for our conjunclizent and max-pooling and propose a new framework classification problems with the state of the system in the correct learning rates of the network is a transformation, and show that for our conjunclizent and max-pooling and propose a new\n",
      "iter3359 : loss2320.64892578\n",
      "he feature of the layers and shortheractions is can be achieved in a different a new representation error on communication of the model compression problems of our pre-training a generative commonol convergence rates of computational striattongris pre-training and deep networks of learning neural ne\n",
      "iter3399 : loss2346.50634766\n",
      "he classification technique to address the capacity of the success of deep neural network to predict the initialize a new approach tore that the network waiks that y each taker of each of the system is the deeperul state-of-the-art methods of a multiplicative emerrest to the size of each onerelient \n",
      "iter3439 : loss2193.20532227\n",
      "he funlaments in the training data from that the variation. The resulting a set of parameter concodding rectifier conrng (paralitied training a general for the input of the network are limities that can be combined with other training and parameters that can be combined with other training and param\n",
      "iter3479 : loss2301.88574219\n",
      "he algorithm is a group of muthunted, but problem, and therefore computes sparse constrained by the accuracy of the interact of some than addition to the someterd is as a generalize works, the context of neural network that deep learning problems on which SVRG to hidden unit as structure than 0.5% o\n",
      "iter3519 : loss2018.91674805\n",
      "he networks and shors of the input of the existence of neural network architectures by a distribution is empirical initializations of conver empirical initializations of conver empirical algorithms have ever approximations of training deep neural networks (DNNs). Triences that are implem inference o\n",
      "iter3559 : loss2364.52294922\n",
      "he conventional layer-wise pre-training a generative model at tasks functions that iterations and they with network propertions of convolutional neural networks with a single layers to learn results on a deep neural networks and have better predictions of convolutional neural networks with a single \n",
      "iter3599 : loss1965.24536133\n",
      "he decomposes the complexity of for most the network depth and subset connections and constraints and the device advantage of an achieves the architectures are intermed from the learning algorithm for case supervision with non-ving boundsof connectly flows from the layer learning to dropout data dis\n",
      "iter3639 : loss2280.89404297\n",
      "he experimental result implementations learned by a fundament convergence tasks. The learned by churnes can be used to extend the recently proposed RNNs (CNN) whys introduced as a method for the experiments led to the system to preserve the model struction performance matching on a single machine le\n",
      "iter3679 : loss2158.71826172\n",
      "he conventional layers, as well as the deep network accuracy of the reasons for test Coffled by the and it is communical speech recognition of learning a deep neural network that these is understanding in the interpretects that a learning in a generalized Bo the the model structure. The information \n",
      "iter3719 : loss2081.0625\n",
      "he classification approaches in a speech recognition in the correntan embeddings of convolutional neural networks (DNN) have droaded provide to the only a regularizer or the simplers of our method can be trained used in the ary finely outher order or which we extend the potential to model compressio\n",
      "iter3759 : loss2050.96533203\n",
      "he framework is speedup with a single layer of the deep learning algorithms to full have even when represent and standard and then state DNN accorviduation in the functions using deep neural networks (DNNs) as the models and weights that which make more experimental result in the learning properties\n",
      "iter3799 : loss2162.9465332\n",
      "he accuracy and compressional convergence than and stochastic most of errory derived as the context of learning algorithms have been the network constrained such as that existing describined by a neural network that introduce a new various and interaction with stochastic gradient descent that conver\n",
      "iter3839 : loss2203.50585938\n",
      "he network depends on the space of adversaries of a set of benchmark structured by a logaritymict that are trained on a simple examples and to a crities of the input shale optimization of a new approach to train time addition technique propostation of different stochastic ropagation or a straight th\n",
      "iter3879 : loss2137.0234375\n",
      "he conventional layer-wise application to computer vision to a pre-training method that our paper preserves the conventional layer-wise pre-training method that our application to complex mapple, with rearal training the model structure of the model as well as important open time of application of t\n",
      "iter3919 : loss2169.78271484\n",
      "he decomposition for designed bootstrap model to an intermed layer is the consider they outperform have novel structure that these networks allows training the desired theoretical results on an learning algorithms to on the soleterculayer on subelis altabistic learnercanis, modevellovica long runt m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-2bd870087a0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_placeholder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatafeeder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_placeholder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_state_placeholder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_placeholder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-5fb4fd0fc253>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(datafeeder, seq_placeholder, init_state_placeholder, out_state, loss, optimizer, sample, temp_placeholder)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatafeeder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mseq_placeholder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "### Run the whole model\n",
    "DATA_PATH = '../data/arvix_abstracts.txt'\n",
    "\n",
    "global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "datafeeder = read_batch(read_data(filename=DATA_PATH))\n",
    "seq_placeholder = tf.placeholder(tf.int32, [None, None], name=\"sequence\")\n",
    "\n",
    "\n",
    "loss, optimizer, init_state_placeholder, out_state, logits = create_model(seq_placeholder,\n",
    "                                                            global_step,\n",
    "                                                            VOCAB_SIZE, \n",
    "                                                            STATE_SIZES, \n",
    "                                                            LR)\n",
    "\n",
    "sample, temp_placeholder = create_generator(logits)\n",
    "\n",
    "sess = train_model(datafeeder, seq_placeholder, init_state_placeholder, out_state, loss, optimizer, sample, temp_placeholder)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}